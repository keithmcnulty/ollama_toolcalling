{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Tools in Llama 3.1 to provide real-time information in an LLM chat\n",
    "\n",
    "Llama 3.1 was just released this week, and supports 'tools'.  Tools are simple functions that you can write and provide to the LLM to answer particular questions.   This can be very valuable in an agentic workflow built for a highly specialized purpose, where you can use the LLM to determine the intent of a user's question in a chat, extract the relevant parameters from the chat and pipe them into the function you have written in order to get the required info.\n",
    "\n",
    "Let's look at a simple example.  I'm going to create a workflow to find out the current temperature in a particular place of interest.\n",
    "\n",
    "### Creating a tool to get the current temperature\n",
    "\n",
    "In this code I am writing a simple Python function which will ping an API to get the current temperature in a place."
   ],
   "id": "1fe29c7a79660903"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T14:56:43.645659Z",
     "start_time": "2024-07-29T14:56:43.258725Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "# function to get the current temperature in a place\n",
    "def get_current_temperature(place: str) -> str:\n",
    "  base_url = f\"https://wttr.in/{place}?format=j1\"\n",
    "  response = requests.get(base_url)\n",
    "  data = response.json()\n",
    "  return f\"The current temperature in {place} is {data['current_condition'][0]['temp_C']} degrees Celsius\"\n",
    "\n",
    "# test the function\n",
    "get_current_temperature(\"London\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in London is 28 degrees Celsius'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That seems to have worked.  \n",
    "\n",
    "### Adding the tool to the LLM workflow\n",
    "\n",
    "Next I am going to use this function as a tool to feed to the LLM.  Not all LLMs support tools, but for those that do, the idea is as follows:\n",
    "* The LLM should identify the intent of a user query and determine that the intent is appropriate to call the tool\n",
    "* The LLM should extract or determine the right arguments/parameters to give to the tool\n",
    "* The LLM should capture the response of the tool and feed it back to the user.  To do this it may have to override some default behaviors (for example many LLMs have default responses which inform users that they cannot provide certain real-time information).\n",
    "\n",
    "To create this workflow, I am going to use an asynchronous chat client via Ollama - I'll leave the model as a parameter so we can try out a few models at the end."
   ],
   "id": "94026eea4e76abed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T14:56:54.409751Z",
     "start_time": "2024-07-29T14:56:54.341080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "import asyncio\n",
    "\n",
    "async def weather_chat(model: str, query: str):\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a query\n",
    "  messages = [{'role': 'user', 'content': query}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  # use the tools parameter to define the tool so the LLM can understand it \n",
    "  response = await client.chat(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    tools = [\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_current_temperature',\n",
    "          'description': 'Get the temperature in a place',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'place': {\n",
    "                'type': 'string',\n",
    "                'description': 'The place for which the temperature is requested',\n",
    "              }\n",
    "            },\n",
    "            'required': ['place'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided tool and warn if it did not\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # If it did use the tool, process tool calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'get_current_temperature': get_current_temperature,\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['place'])\n",
    "      # Add function response to the conversation\n",
    "      # Warn model to override any default behavior\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': f\"\"\"\n",
    "          Ignore any other information and use only this information, if relevant, to answer the original question.  {function_response}\n",
    "          \"\"\",\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  # Hopefully it will provide the information obtained using the tool\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])\n"
   ],
   "id": "1d5b08244610167f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing using various open models and queries\n",
    "First, let's try this with Google's Gemma2 model and a simple question.\n"
   ],
   "id": "775606c24ada6b42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:05:10.308122Z",
     "start_time": "2024-07-29T15:05:10.040248Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('gemma2:27b', 'What is the temperature like right now in Dublin?')",
   "id": "2613434018146722",
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "gemma2:27b does not support tools",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResponseError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m weather_chat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgemma2:27b\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWhat is the temperature like right now in Dublin?\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 11\u001B[0m, in \u001B[0;36mweather_chat\u001B[0;34m(model, query)\u001B[0m\n\u001B[1;32m      7\u001B[0m messages \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m: query}]\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# First API call: Send the query and function description to the model\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# use the tools parameter to define the tool so the LLM can understand it \u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m client\u001B[38;5;241m.\u001B[39mchat(\n\u001B[1;32m     12\u001B[0m   model \u001B[38;5;241m=\u001B[39m model,\n\u001B[1;32m     13\u001B[0m   messages \u001B[38;5;241m=\u001B[39m messages,\n\u001B[1;32m     14\u001B[0m   tools \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     15\u001B[0m     {\n\u001B[1;32m     16\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunction\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     17\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunction\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mget_current_temperature\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGet the temperature in a place\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparameters\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     21\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     22\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproperties\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     23\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplace\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     24\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     25\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe place for which the temperature is requested\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     26\u001B[0m             }\n\u001B[1;32m     27\u001B[0m           },\n\u001B[1;32m     28\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrequired\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplace\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     29\u001B[0m         },\n\u001B[1;32m     30\u001B[0m       },\n\u001B[1;32m     31\u001B[0m     },\n\u001B[1;32m     32\u001B[0m   ],\n\u001B[1;32m     33\u001B[0m )\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Add the model's response to the conversation history\u001B[39;00m\n\u001B[1;32m     36\u001B[0m messages\u001B[38;5;241m.\u001B[39mappend(response[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:653\u001B[0m, in \u001B[0;36mAsyncClient.chat\u001B[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001B[0m\n\u001B[1;32m    650\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m images \u001B[38;5;241m:=\u001B[39m message\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    651\u001B[0m     message[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [_encode_image(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m--> 653\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request_stream(\n\u001B[1;32m    654\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    655\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/api/chat\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    656\u001B[0m   json\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    657\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: model,\n\u001B[1;32m    658\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m'\u001B[39m: messages,\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m'\u001B[39m: tools \u001B[38;5;129;01mor\u001B[39;00m [],\n\u001B[1;32m    660\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m'\u001B[39m: stream,\n\u001B[1;32m    661\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mformat\u001B[39m,\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptions\u001B[39m\u001B[38;5;124m'\u001B[39m: options \u001B[38;5;129;01mor\u001B[39;00m {},\n\u001B[1;32m    663\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeep_alive\u001B[39m\u001B[38;5;124m'\u001B[39m: keep_alive,\n\u001B[1;32m    664\u001B[0m   },\n\u001B[1;32m    665\u001B[0m   stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    666\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:517\u001B[0m, in \u001B[0;36mAsyncClient._request_stream\u001B[0;34m(self, stream, *args, **kwargs)\u001B[0m\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    515\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 517\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:487\u001B[0m, in \u001B[0;36mAsyncClient._request\u001B[0;34m(self, method, url, **kwargs)\u001B[0m\n\u001B[1;32m    485\u001B[0m   response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    486\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 487\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext, e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[0;31mResponseError\u001B[0m: gemma2:27b does not support tools"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, so we can see that some models don't yet support this feature.  Let's try the small version of Llama 3.1.",
   "id": "b13c1d99708621d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:06:30.186497Z",
     "start_time": "2024-07-29T15:06:20.433215Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', 'What is the temperature like right now in Dublin?')",
   "id": "ab534669bbcba3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't provide real-time information or access external data. However, I can suggest some ways for you to find out the current temperature in Dublin.\n",
      "\n",
      "1. Check online weather websites such as AccuWeather or Weather.com.\n",
      "2. Use a search engine like Google and type \"current temperature in Dublin\" to get the latest updates.\n",
      "3. Look up the official website of the Irish Meteorological Service (Met Éireann) for the most accurate and up-to-date information.\n",
      "\n",
      "Please note that temperatures can vary depending on the source and location within Dublin, so it's always a good idea to check multiple sources for the most accurate information.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK so the small version of LLama 3.1 has not been able to recognise the intent of the question and use my function, and hence defaults to it's standard behavior when asked for real time information or external data.  Let's try a larger version of Llama 3.1.",
   "id": "cf938bbaa66f03b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:08:07.795058Z",
     "start_time": "2024-07-29T15:07:59.674922Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:70b', 'What is the temperature like right now in Dublin?')",
   "id": "4cc7f159adffe0ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Dublin is 21 degrees Celsius.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Oh, that's cool!  Llama 70b is able to handle my workflow.  So what happens if we make the question a bit less direct?",
   "id": "f5da40c288915796"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:09:52.624617Z",
     "start_time": "2024-07-29T15:09:43.408050Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:70b', \"What is the temperature like in Ireland's capital city?\")",
   "id": "79fa0bc037ce5e84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Dublin, Ireland's capital city, is 21 degrees Celsius.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Very nice! Let's push things a little further.",
   "id": "6e80545caf94cf10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:12:43.822879Z",
     "start_time": "2024-07-29T15:12:27.824929Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:70b', \"My sister says that if I went flew into the capital City of Norway today, I should dress for a very cold temperature less than 5 degrees Celsius.  Should I trust her advice?\")",
   "id": "3d1b1dc68db6cac1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided by the weather API, the current temperature in Oslo, Norway is 21 degrees Celsius. Therefore, your sister's advice to dress for a very cold temperature less than 5 degrees Celsius is incorrect. You should not trust her advice in this case. It would be more suitable to dress for mild or warm weather instead.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T15:18:17.269557Z",
     "start_time": "2024-07-29T15:17:58.747233Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:70b', \"Compare the temperatures of these two cities right now: Dunedin, New Zealand and Reykjavik, Iceland?\")",
   "id": "52a713e58b780756",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Dunedin, New Zealand is 5 degrees Celsius, while the current temperature in Reykjavik, Iceland is 13 degrees Celsius. Therefore, it is currently warmer in Reykjavik, Iceland than in Dunedin, New Zealand by a difference of 8 degrees Celsius.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5d02f88937edc7df"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
