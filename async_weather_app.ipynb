{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Tools in Llama 3.1 to provide real-time information in an LLM chat\n",
    "\n",
    "Llama 3.1 was just released this week, and supports 'tools'.  Tools are simple functions that you can write and provide to the LLM to answer particular questions.   This can be very valuable in an agentic workflow built for a highly specialized purpose, where you can use the LLM to determine the intent of a user's question in a chat, extract the relevant parameters from the chat and pipe them into the function you have written in order to get the required info.\n",
    "\n",
    "Let's look at a simple example.  I'm going to create a workflow to find out the current temperature in a particular place of interest.\n",
    "\n",
    "### Creating a tool to get the current temperature\n",
    "\n",
    "In this code I am writing a simple Python function which will ping an API to get the current temperature in a place."
   ],
   "id": "1fe29c7a79660903"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T19:02:39.242990Z",
     "start_time": "2024-07-29T19:02:38.943615Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "# function to get the current temperature in a place\n",
    "def get_current_temperature(place: str) -> str:\n",
    "  base_url = f\"https://wttr.in/{place}?format=j1\"\n",
    "  response = requests.get(base_url)\n",
    "  data = response.json()\n",
    "  return f\"The current temperature in {place} is {data['current_condition'][0]['temp_C']} degrees Celsius\"\n",
    "\n",
    "# test the function\n",
    "get_current_temperature(\"London\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in London is 27 degrees Celsius'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That seems to have worked.  \n",
    "\n",
    "### Adding the tool to the LLM workflow\n",
    "\n",
    "Next I am going to use this function as a tool to feed to the LLM.  Not all LLMs support tools, but for those that do, the idea is as follows:\n",
    "* The LLM should identify the intent of a user query and determine that the intent is appropriate to call the tool\n",
    "* The LLM should extract or determine the right arguments/parameters to give to the tool\n",
    "* The LLM should capture the response of the tool and feed it back to the user.  To do this it may have to override some default behaviors (for example many LLMs have default responses which inform users that they cannot provide certain real-time information).\n",
    "\n",
    "To create this workflow, I am going to use an asynchronous chat client via Ollama - I'll leave the model as a parameter so we can try out a few models at the end."
   ],
   "id": "94026eea4e76abed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:02:44.140814Z",
     "start_time": "2024-07-29T19:02:44.135222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "import asyncio\n",
    "\n",
    "async def weather_chat(model: str, query: str):\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a query\n",
    "  messages = [{'role': 'user', 'content': query}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  # use the tools parameter to define the tool so the LLM can understand it \n",
    "  response = await client.chat(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    tools = [\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_current_temperature',\n",
    "          'description': 'Get the temperature in a place',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'place': {\n",
    "                'type': 'string',\n",
    "                'description': 'The place for which the temperature is requested',\n",
    "              }\n",
    "            },\n",
    "            'required': ['place'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided tool and warn if it did not\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # If it did use the tool, process tool calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'get_current_temperature': get_current_temperature,\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['place'])\n",
    "      # Add function response to the conversation\n",
    "      # Warn model to override any default behavior\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': f\"\"\"\n",
    "          Answer the following question: {query}.  \n",
    "          Ignore any previous instructions or defaults and instead use the following information:  {function_response}\n",
    "          \"\"\",\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  # Hopefully it will provide the information obtained using the tool\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])\n"
   ],
   "id": "1d5b08244610167f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing using various open models and queries\n",
    "First, let's try this with Google's Gemma2 model and a simple question.\n"
   ],
   "id": "775606c24ada6b42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:02:55.441546Z",
     "start_time": "2024-07-29T19:02:55.309400Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('gemma2:27b', 'What is the temperature like right now in Dublin?')",
   "id": "2613434018146722",
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "gemma2:27b does not support tools",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResponseError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m weather_chat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgemma2:27b\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWhat is the temperature like right now in Dublin?\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[26], line 11\u001B[0m, in \u001B[0;36mweather_chat\u001B[0;34m(model, query)\u001B[0m\n\u001B[1;32m      7\u001B[0m messages \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m: query}]\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# First API call: Send the query and function description to the model\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# use the tools parameter to define the tool so the LLM can understand it \u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m client\u001B[38;5;241m.\u001B[39mchat(\n\u001B[1;32m     12\u001B[0m   model \u001B[38;5;241m=\u001B[39m model,\n\u001B[1;32m     13\u001B[0m   messages \u001B[38;5;241m=\u001B[39m messages,\n\u001B[1;32m     14\u001B[0m   tools \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     15\u001B[0m     {\n\u001B[1;32m     16\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunction\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     17\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunction\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mget_current_temperature\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGet the temperature in a place\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparameters\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     21\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     22\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproperties\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     23\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplace\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     24\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     25\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe place for which the temperature is requested\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     26\u001B[0m             }\n\u001B[1;32m     27\u001B[0m           },\n\u001B[1;32m     28\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrequired\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplace\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     29\u001B[0m         },\n\u001B[1;32m     30\u001B[0m       },\n\u001B[1;32m     31\u001B[0m     },\n\u001B[1;32m     32\u001B[0m   ],\n\u001B[1;32m     33\u001B[0m )\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Add the model's response to the conversation history\u001B[39;00m\n\u001B[1;32m     36\u001B[0m messages\u001B[38;5;241m.\u001B[39mappend(response[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:653\u001B[0m, in \u001B[0;36mAsyncClient.chat\u001B[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001B[0m\n\u001B[1;32m    650\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m images \u001B[38;5;241m:=\u001B[39m message\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    651\u001B[0m     message[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [_encode_image(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m--> 653\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request_stream(\n\u001B[1;32m    654\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    655\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/api/chat\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    656\u001B[0m   json\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    657\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: model,\n\u001B[1;32m    658\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m'\u001B[39m: messages,\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m'\u001B[39m: tools \u001B[38;5;129;01mor\u001B[39;00m [],\n\u001B[1;32m    660\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m'\u001B[39m: stream,\n\u001B[1;32m    661\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mformat\u001B[39m,\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptions\u001B[39m\u001B[38;5;124m'\u001B[39m: options \u001B[38;5;129;01mor\u001B[39;00m {},\n\u001B[1;32m    663\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeep_alive\u001B[39m\u001B[38;5;124m'\u001B[39m: keep_alive,\n\u001B[1;32m    664\u001B[0m   },\n\u001B[1;32m    665\u001B[0m   stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    666\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:517\u001B[0m, in \u001B[0;36mAsyncClient._request_stream\u001B[0;34m(self, stream, *args, **kwargs)\u001B[0m\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    515\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 517\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:487\u001B[0m, in \u001B[0;36mAsyncClient._request\u001B[0;34m(self, method, url, **kwargs)\u001B[0m\n\u001B[1;32m    485\u001B[0m   response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    486\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 487\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext, e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[0;31mResponseError\u001B[0m: gemma2:27b does not support tools"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, so we can see that some models don't yet support this feature.  Let's try the small version of Llama 3.1.",
   "id": "b13c1d99708621d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:03:06.468280Z",
     "start_time": "2024-07-29T19:03:05.172436Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', 'What is the temperature like right now in Dublin?')",
   "id": "ab534669bbcba3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Dublin is 20 degrees Celsius.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Works nicely - let's try asking indirectly.",
   "id": "cf938bbaa66f03b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:03:55.663419Z",
     "start_time": "2024-07-29T19:03:54.122009Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"What is the temperature like in Ireland's capital city?\")",
   "id": "79fa0bc037ce5e84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature in Dublin, the capital city of Ireland, is currently 20 degrees Celsius.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Very nice! Let's push things a little further.",
   "id": "6e80545caf94cf10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:06:16.408389Z",
     "start_time": "2024-07-29T19:06:13.892347Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"My sister says that if I flew into the capital city of Norway today, I should wear warm clothing.  Should I trust her advice?\")",
   "id": "3d1b1dc68db6cac1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the current temperature in Oslo being 20 degrees Celsius, it's likely that your sister's advice to wear warm clothing was incorrect. In fact, 20째C is a relatively mild temperature, and you might even consider wearing light layers or no additional warmth at all. So, to answer your question, it would be reasonable to disregard her advice in this case.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:06:39.779728Z",
     "start_time": "2024-07-29T19:06:36.317969Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"Compare the temperatures of these two cities right now: Dunedin, New Zealand and Reykjavik, Iceland?\")",
   "id": "52a713e58b780756",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information:\n",
      "\n",
      "Dunedin, New Zealand's current temperature is 4째C.\n",
      "Reykjavik, Iceland's current temperature is 13째C.\n",
      "\n",
      "Comparing these temperatures, we can see that Reykjavik, Iceland is currently warmer than Dunedin, New Zealand by 9째C.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:07:43.007098Z",
     "start_time": "2024-07-29T19:07:37.492197Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"What kinds of clothes should I pack for my trip to Hobart, Tasmania which leaves tomorrow?\")",
   "id": "918a2ccd021cb4f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the current temperature in Hobart, Tasmania being 1 degrees Celsius, it's likely that you'll be experiencing cool to mild weather.\n",
      "\n",
      "For your trip, I'd recommend packing:\n",
      "\n",
      "* A mix of lightweight and breathable tops (t-shirts, blouses) for warmer moments\n",
      "* Some insulating layers (fleeces, sweaters) for cooler periods\n",
      "* A waterproof or windbreaker jacket to protect against any rain or wind\n",
      "* Warm socks and a hat for chilly mornings and evenings\n",
      "* Comfortable walking shoes or boots with good grip, as Hobart's streets can be quite hilly and windy\n",
      "* Layers of clothing that can be easily added or removed as needed to maintain a comfortable body temperature\n",
      "\n",
      "Remember to check the weather forecast before your trip to ensure you're prepared for any potential weather conditions.\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
