{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Tools in Llama 3.1 to provide real-time information in an LLM chat\n",
    "\n",
    "Llama 3.1 was just released this week, and supports 'tools'.  Tools are simple functions that you can write and provide to the LLM to answer particular questions.   This can be very valuable in an agentic workflow built for a highly specialized purpose, where you can use the LLM to determine the intent of a user's question in a chat, extract the relevant parameters from the chat and pipe them into the function you have written in order to get the required info.\n",
    "\n",
    "Let's look at a simple example.  I'm going to create a workflow to find out the current temperature in a particular place of interest.\n",
    "\n",
    "### Creating a tool to get the current temperature\n",
    "\n",
    "In this code I am writing a simple Python function which will ping an API to get the current temperature in a place."
   ],
   "id": "1fe29c7a79660903"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T23:05:11.667151Z",
     "start_time": "2024-09-10T23:05:07.923912Z"
    }
   },
   "source": [
    "import requests\n",
    "\n",
    "# function to get the current temperature in a place\n",
    "def get_current_temperature(place: str) -> str:\n",
    "  base_url = f\"https://wttr.in/{place}?format=j1\"\n",
    "  response = requests.get(base_url)\n",
    "  data = response.json()\n",
    "  return f\"The current temperature in {place} is {data['current_condition'][0]['temp_C']} degrees Celsius\"\n",
    "\n",
    "# test the function\n",
    "get_current_temperature(\"London\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature in London is 10 degrees Celsius'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That seems to have worked.  \n",
    "\n",
    "### Adding the tool to the LLM workflow\n",
    "\n",
    "Next I am going to use this function as a tool to feed to the LLM.  Not all LLMs support tools, but for those that do, the idea is as follows:\n",
    "* The LLM should identify the intent of a user query and determine that the intent is appropriate to call the tool\n",
    "* The LLM should extract or determine the right arguments/parameters to give to the tool\n",
    "* The LLM should capture the response of the tool and feed it back to the user.  To do this it may have to override some default behaviors (for example many LLMs have default responses which inform users that they cannot provide certain real-time information).\n",
    "\n",
    "First, let's send a request with a message to Llama 3.1, and as part of that request, we will inform Llama that we have a function on our end which can determine the weather in a given place."
   ],
   "id": "94026eea4e76abed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T23:05:18.299944Z",
     "start_time": "2024-09-10T23:05:17.881352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "client = ollama.Client()\n",
    "\n",
    "def tool_chat(model: str, query: str):\n",
    "    response = client.chat(\n",
    "        model = model,\n",
    "        messages = [{'role': 'user', 'content': query}],\n",
    "        tools = [\n",
    "          {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "              'name': 'get_current_temperature',\n",
    "              'description': 'Get the temperature in a place',\n",
    "              'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                  'place': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The place for which the temperature is requested',\n",
    "                  }\n",
    "                },\n",
    "                'required': ['place'],\n",
    "              },\n",
    "            },\n",
    "          },\n",
    "        ],\n",
    "      )\n",
    "    \n",
    "    return response['message']"
   ],
   "id": "929ea0bf4631c31f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's see what the LLM returns when we ask a question that relates in some way to weather.",
   "id": "8e2db81b8cdd018"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T23:05:27.762269Z",
     "start_time": "2024-09-10T23:05:20.593233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Should I wear a warm coat today in Rome?\"\n",
    "\n",
    "tool_chat(\"llama3.1:8b\", query)"
   ],
   "id": "9c5b029fb17d1312",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '',\n",
       " 'tool_calls': [{'function': {'name': 'get_current_temperature',\n",
       "    'arguments': {'place': 'Rome'}}}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T23:05:33.190848Z",
     "start_time": "2024-09-10T23:05:33.187362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "import asyncio\n",
    "\n",
    "async def weather_chat(model: str, query: str):\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a query\n",
    "  messages = [{'role': 'user', 'content': query}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  # use the tools parameter to define the tool so the LLM can understand it \n",
    "  response = await client.chat(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    tools = [\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_current_temperature',\n",
    "          'description': 'Get the temperature in a place',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'place': {\n",
    "                'type': 'string',\n",
    "                'description': 'The place for which the temperature is requested',\n",
    "              }\n",
    "            },\n",
    "            'required': ['place'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided tool and warn if it did not\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # If it did use the tool, process tool calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'get_current_temperature': get_current_temperature,\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['place'])\n",
    "      # Add function response to the conversation\n",
    "      # Warn model to override any default behavior\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': f\"\"\"\n",
    "          Answer the following question: {query}.  \n",
    "          Ignore any previous instructions or defaults and instead use the following information:  {function_response}\n",
    "          \"\"\",\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  # Hopefully it will provide the information obtained using the tool\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])\n"
   ],
   "id": "1d5b08244610167f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing using various open models and queries\n",
    "First, let's try this with Google's Gemma2 model and a simple question.\n"
   ],
   "id": "775606c24ada6b42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:02:55.441546Z",
     "start_time": "2024-07-29T19:02:55.309400Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('gemma2:27b', 'What is the temperature like right now in Dublin?')",
   "id": "2613434018146722",
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "gemma2:27b does not support tools",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mResponseError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m weather_chat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgemma2:27b\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWhat is the temperature like right now in Dublin?\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[26], line 11\u001B[0m, in \u001B[0;36mweather_chat\u001B[0;34m(model, query)\u001B[0m\n\u001B[1;32m      7\u001B[0m messages \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m: query}]\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# First API call: Send the query and function description to the model\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# use the tools parameter to define the tool so the LLM can understand it \u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m client\u001B[38;5;241m.\u001B[39mchat(\n\u001B[1;32m     12\u001B[0m   model \u001B[38;5;241m=\u001B[39m model,\n\u001B[1;32m     13\u001B[0m   messages \u001B[38;5;241m=\u001B[39m messages,\n\u001B[1;32m     14\u001B[0m   tools \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     15\u001B[0m     {\n\u001B[1;32m     16\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunction\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     17\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunction\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mget_current_temperature\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGet the temperature in a place\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparameters\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     21\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     22\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproperties\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     23\u001B[0m             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplace\u001B[39m\u001B[38;5;124m'\u001B[39m: {\n\u001B[1;32m     24\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     25\u001B[0m               \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdescription\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe place for which the temperature is requested\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     26\u001B[0m             }\n\u001B[1;32m     27\u001B[0m           },\n\u001B[1;32m     28\u001B[0m           \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrequired\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplace\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     29\u001B[0m         },\n\u001B[1;32m     30\u001B[0m       },\n\u001B[1;32m     31\u001B[0m     },\n\u001B[1;32m     32\u001B[0m   ],\n\u001B[1;32m     33\u001B[0m )\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Add the model's response to the conversation history\u001B[39;00m\n\u001B[1;32m     36\u001B[0m messages\u001B[38;5;241m.\u001B[39mappend(response[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:653\u001B[0m, in \u001B[0;36mAsyncClient.chat\u001B[0;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001B[0m\n\u001B[1;32m    650\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m images \u001B[38;5;241m:=\u001B[39m message\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    651\u001B[0m     message[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m [_encode_image(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m--> 653\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request_stream(\n\u001B[1;32m    654\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    655\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/api/chat\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    656\u001B[0m   json\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m    657\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: model,\n\u001B[1;32m    658\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m'\u001B[39m: messages,\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m'\u001B[39m: tools \u001B[38;5;129;01mor\u001B[39;00m [],\n\u001B[1;32m    660\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m'\u001B[39m: stream,\n\u001B[1;32m    661\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mformat\u001B[39m,\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptions\u001B[39m\u001B[38;5;124m'\u001B[39m: options \u001B[38;5;129;01mor\u001B[39;00m {},\n\u001B[1;32m    663\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeep_alive\u001B[39m\u001B[38;5;124m'\u001B[39m: keep_alive,\n\u001B[1;32m    664\u001B[0m   },\n\u001B[1;32m    665\u001B[0m   stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    666\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:517\u001B[0m, in \u001B[0;36mAsyncClient._request_stream\u001B[0;34m(self, stream, *args, **kwargs)\u001B[0m\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    515\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 517\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ollama_toolcalling/lib/python3.10/site-packages/ollama/_client.py:487\u001B[0m, in \u001B[0;36mAsyncClient._request\u001B[0;34m(self, method, url, **kwargs)\u001B[0m\n\u001B[1;32m    485\u001B[0m   response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    486\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mHTTPStatusError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 487\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext, e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[0;31mResponseError\u001B[0m: gemma2:27b does not support tools"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, so we can see that some models don't yet support this feature.  Let's try the small version of Llama 3.1.",
   "id": "b13c1d99708621d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:05:12.427553Z",
     "start_time": "2024-07-29T20:05:10.308975Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', 'What is the temperature like right now in Dublin?')",
   "id": "ab534669bbcba3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Dublin is 20 degrees Celsius.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Works nicely - let's try asking indirectly.",
   "id": "cf938bbaa66f03b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:12:36.929802Z",
     "start_time": "2024-07-29T20:12:34.541290Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"What is the current temperature in Ireland's capital?\")",
   "id": "79fa0bc037ce5e84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided answer, I can tell you that the current temperature in Dublin, the capital of Ireland, is 20 degrees Celsius.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Very nice! Let's push things a little further.",
   "id": "6e80545caf94cf10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T20:24:13.926803Z",
     "start_time": "2024-08-19T20:24:01.334731Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"My sister says that if I flew into the capital city of Norway today, I should wear clothing for extreme weather.  Should I trust her advice?\")",
   "id": "3d1b1dc68db6cac1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's generally a good idea to trust your sister's advice, especially if she has experience with extreme weather conditions. However, in this case, the current temperature in Oslo is 13 degrees Celsius, which is quite mild. If you're planning to visit Oslo today, it's unlikely that you'll need to wear clothing for extreme weather. A light jacket or sweater should be sufficient.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:22:08.078438Z",
     "start_time": "2024-07-29T20:22:04.509681Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"Compare the temperatures of these two cities right now: Dunedin, New Zealand and Reykjavik, Iceland?\")",
   "id": "52a713e58b780756",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, I can compare the temperatures of the two cities as follows:\n",
      "\n",
      "Dunedin, New Zealand: 4°C (current temperature)\n",
      "Reykjavik, Iceland: 13°C (current temperature)\n",
      "\n",
      "The temperature in Reykjavik, Iceland is currently higher than that of Dunedin, New Zealand by 9 degrees Celsius.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:23:42.287745Z",
     "start_time": "2024-07-29T20:23:37.679098Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"What kinds of clothes should I pack for my trip to Tasmania which leaves tomorrow?\")",
   "id": "918a2ccd021cb4f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering the current temperature in Tasmania is 3°C, I would recommend packing layers for your trip. It's likely to be cool and possibly even chilly, especially in the mornings and evenings.\n",
      "\n",
      "For a comfortable and versatile wardrobe, consider packing:\n",
      "\n",
      "* A mix of lightweight and warmer tops (fleeces, sweaters, or thermals)\n",
      "* Waterproof or water-resistant outerwear (jacket or windbreaker) to protect against Tasmania's infamous rain\n",
      "* Comfortable and sturdy pants or trousers for outdoor activities like hiking or exploring\n",
      "* Insulating layers (fleece or down jacket) for colder moments\n",
      "* Warm socks and gloves for chilly mornings and evenings\n",
      "* A scarf or neck warmer for added warmth\n",
      "* Waterproof shoes or boots with good grip for navigating Tasmania's rugged terrain\n",
      "\n",
      "Remember to check the weather forecast before your trip to ensure you're prepared for any potential changes in temperature or precipitation. Have a great time in Tasmania!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T21:21:33.215469Z",
     "start_time": "2024-07-29T21:21:23.491567Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"How much longer would it take a 50g ice cube to melt today in Dunedin compared to Marrakech?\")\n",
   "id": "bc2c4217fd90160",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To answer this question, we need to calculate the time it would take for a 50g ice cube to melt at each location.\n",
      "\n",
      "The rate of melting is directly proportional to the temperature difference between the object and its surroundings. We can use the following formula to estimate the melting time:\n",
      "\n",
      "Melting Time = (mass x specific heat capacity) / (density x surface area x temperature difference)\n",
      "\n",
      "We'll assume a density of 0.92 g/cm³ for ice, a specific heat capacity of 2.05 J/g°C, and a surface area of approximately 6 cm² for the 50g ice cube.\n",
      "\n",
      "First, let's calculate the melting time in Dunedin:\n",
      "\n",
      "Temperature difference = 4°C (Dunedin) - 0°C (ice) = 4°C\n",
      "Melting Time in Dunedin ≈ (50 g x 2.05 J/g°C) / (0.92 g/cm³ x 6 cm² x 4°C) ≈ 137 seconds\n",
      "\n",
      "Next, let's calculate the melting time in Marrakech:\n",
      "\n",
      "Temperature difference = 27°C (Marrakech) - 0°C (ice) = 27°C\n",
      "Melting Time in Marrakech ≈ (50 g x 2.05 J/g°C) / (0.92 g/cm³ x 6 cm² x 27°C) ≈ 10 seconds\n",
      "\n",
      "Now, let's find the difference in melting times:\n",
      "\n",
      "Difference in Melting Times ≈ 137 seconds - 10 seconds ≈ 127 seconds\n",
      "\n",
      "Therefore, it would take a 50g ice cube approximately 127 seconds longer to melt today in Dunedin compared to Marrakech.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T21:28:48.980991Z",
     "start_time": "2024-07-29T21:26:42.454047Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:70b', \"How much longer would it take a 50g ice cube to melt today in Dunedin compared to Marrakech?\")",
   "id": "617d67a06405d4f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate how much longer it would take for a 50g ice cube to melt in Dunedin compared to Marrakech, we need to consider the temperature difference between the two locations.\n",
      "\n",
      "Assuming the melting point of ice is around 0°C (32°F), and using the temperatures you provided:\n",
      "\n",
      "* In Dunedin: 4°C\n",
      "* In Marrakech: 27°C\n",
      "\n",
      "The temperature difference between the two locations is:\n",
      "\n",
      "27°C - 4°C = 23°C\n",
      "\n",
      "This means that the air in Marrakech is approximately 23°C warmer than the air in Dunedin.\n",
      "\n",
      "Now, let's consider the melting rate of ice. The exact melting rate depends on various factors, including the shape and size of the ice cube, air circulation, and humidity. However, as a rough estimate, we can assume that the melting rate of ice is roughly proportional to the temperature difference between the ice and the surrounding air.\n",
      "\n",
      "Using this assumption, we can estimate the relative melting rates in Dunedin and Marrakech:\n",
      "\n",
      "* In Dunedin: 4°C (relatively slow melting)\n",
      "* In Marrakech: 27°C (relatively fast melting)\n",
      "\n",
      "Since the temperature difference is 23°C, we can assume that the ice cube would melt approximately 5.75 times faster in Marrakech than in Dunedin (23°C / 4°C).\n",
      "\n",
      "To calculate how much longer it would take for the ice cube to melt in Dunedin compared to Marrakech, we can use the following formula:\n",
      "\n",
      "Time difference = Time taken to melt in Dunedin - Time taken to melt in Marrakech\n",
      "\n",
      "Assuming the ice cube takes approximately 1 hour to melt in Marrakech (a rough estimate), it would take around 5.75 hours to melt in Dunedin.\n",
      "\n",
      "Therefore, it would take approximately 4.75 hours longer for a 50g ice cube to melt today in Dunedin compared to Marrakech.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-19T23:02:05.607608Z",
     "start_time": "2024-08-19T23:01:37.830899Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:70b', \"If a wombat was transported today to Northern Sweden and a penguin was transported today to Singapore, which would have the best chance of survival?\")",
   "id": "3bb47823431f8ade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the temperatures provided, I would say that the wombat has a better chance of survival. Wombats are native to Australia and are adapted to temperate climates with mild winters and cool summers. Northern Sweden's temperature of 12°C (54°F) is within the wombat's tolerance range.\n",
      "\n",
      "On the other hand, penguins are found in cold climates, such as Antarctica and the surrounding islands. Singapore's temperature of 28°C (82°F) is much too hot for a penguin, which would likely suffer from heat stress and dehydration.\n",
      "\n",
      "Therefore, I believe that the wombat has a better chance of survival in Northern Sweden than the penguin does in Singapore.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding multiple functions\n",
    "\n",
    "Let's construct a second function which obtains details of events available on Ticketmaster."
   ],
   "id": "f96388fc783d9640"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T11:04:52.492043Z",
     "start_time": "2024-09-11T11:04:52.123533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function to get the today's sunset time in a place\n",
    "def get_sunset(place: str) -> str:\n",
    "  base_url = f'https://wttr.in/{place}?format=\"%s\"'\n",
    "  response = requests.get(base_url)\n",
    "  data = response.json()\n",
    "  return f\"Today's sunset time in {place} is {data}\"\n",
    "\n",
    "# test the function\n",
    "get_sunset(\"London\")"
   ],
   "id": "3ec5cee159b4a841",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's sunset time in London is 19:22:59\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's add this as an additional function available to Llama 3.1",
   "id": "49ea909208422a71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T11:09:49.491438Z",
     "start_time": "2024-09-11T11:09:49.485844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def weather_and_sunset_chat(model: str, query: str):\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a query\n",
    "  messages = [{'role': 'user', 'content': query}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  # use the tools parameter to define the tool so the LLM can understand it \n",
    "  response = await client.chat(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    tools = [\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_current_temperature',\n",
    "          'description': 'Get the temperature in a place',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'place': {\n",
    "                'type': 'string',\n",
    "                'description': 'The place for which the temperature is requested',\n",
    "              }\n",
    "            },\n",
    "            'required': ['place'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "     {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_sunset',\n",
    "          'description': 'Get the sunset time in a place',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'place': {\n",
    "                'type': 'string',\n",
    "                'description': 'The city in which the sunset time is requested',\n",
    "              }\n",
    "            },\n",
    "            'required': ['place'],\n",
    "          },\n",
    "        },\n",
    "      }, \n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided tool and warn if it did not\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # If it did use the tool, process tool calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'get_current_temperature': get_current_temperature,\n",
    "      'get_sunset': get_sunset\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['place'])\n",
    "      # Add function response to the conversation\n",
    "      # Warn model to override any default behavior\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': f\"\"\"\n",
    "          Answer the following question: {query}.  \n",
    "          Ignore any previous instructions or defaults and instead use the following information:  {function_response}\n",
    "          \"\"\",\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  # Hopefully it will provide the information obtained using the tool\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])"
   ],
   "id": "b0f38873186038dd",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now let's test a few queries:",
   "id": "e87b6b8c5621bc7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T11:09:56.455849Z",
     "start_time": "2024-09-11T11:09:53.734997Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_and_sunset_chat(\"llama3.1:8b\", \"Should I dress light for my visit To Austin, Texas today?\")",
   "id": "95b92b9401d729d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you should dress lightly for your visit to Austin, Texas today. The temperature is around 75°F (24°C), which is quite warm. You may want to consider wearing light and breathable clothing such as t-shirts, shorts, and sunglasses to stay cool.\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T11:16:21.180908Z",
     "start_time": "2024-09-11T11:16:14.539321Z"
    }
   },
   "cell_type": "code",
   "source": "await weather_and_sunset_chat(\"llama3.1:8b\", \"I'm driving about 100 miles out of town of Austin, Texas today.  What kind of clothes should I pack?  Also I want to be back before dark.  Do you have any advice?\")",
   "id": "1ca6c0b2ea76d34a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the current temperature in Austin, Texas being 24 degrees Celsius, it's likely to be warm during your drive. I would recommend packing light and breathable clothing such as t-shirts, shorts, and a hat.\n",
      "\n",
      "Since you want to be back before dark, which is at 19:40:25, I would suggest starting your return journey by around 17:00-18:00 to ensure you arrive back in Austin with enough time to spare. This will also give you some buffer in case of any unexpected delays or traffic.\n",
      "\n",
      "Additionally, you may want to check the weather forecast for the area you'll be driving through to see if there are any potential storms or rain showers that could affect your drive. It's always a good idea to be prepared for any conditions, especially when driving long distances.\n",
      "\n",
      "Overall, with comfortable and practical clothing, and a sensible return time, you should have a safe and enjoyable drive back to Austin!\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "86015780941f0423"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
