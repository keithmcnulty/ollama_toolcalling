{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Tools/Function Calling in Llama 3.1 to provide real-time information in an LLM chat\n",
    "\n",
    "Llama 3.1 supports 'tools'.  Tools are  functions that you can write and provide to the LLM to answer particular questions.   This can be very valuable in an agentic workflow built for a highly specialized purpose, where you can use the LLM to determine the intent of a user's question in a chat, extract the relevant parameters from the chat and pipe them into the function you have written in order to get the required info.\n",
    "\n",
    "![Tools/Function Calling in Llama 3.1](function_calling.png)\n",
    "\n",
    "Let's look at a simple example.  I'm going to create a workflow to find out the current temperature in a particular place of interest.  I'll then expand the tooling workflow to also allow the LLM to have access to the sunset time in a particular place.\n",
    "\n",
    "### Creating a tool to get the current temperature\n",
    "\n",
    "In this code I am writing a simple Python function which will ping an API to get the current temperature in a place."
   ],
   "id": "1fe29c7a79660903"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import requests\n",
    "\n",
    "# function to get the current temperature in a place\n",
    "def get_current_temperature(place: str) -> str:\n",
    "  base_url = f\"https://wttr.in/{place}?format=j1\"\n",
    "  response = requests.get(base_url)\n",
    "  data = response.json()\n",
    "  return f\"The current temperature in {place} is {data['current_condition'][0]['temp_C']} degrees Celsius\"\n",
    "\n",
    "# test the function\n",
    "get_current_temperature(\"London\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "That seems to have worked.  \n",
    "\n",
    "### Adding the tool to the LLM workflow\n",
    "\n",
    "Next I am going to use this function as a tool to feed to the LLM.  Not all LLMs support tools, but for those that do, the idea is as follows:\n",
    "* The LLM should identify the intent of a user query and determine that the intent is appropriate to call the tool\n",
    "* The LLM should extract or determine the right arguments/parameters to give to the tool\n",
    "* The LLM should capture the response of the tool and feed it back to the user.  To do this it may have to override some default behaviors (for example many LLMs have default responses which inform users that they cannot provide certain real-time information).\n",
    "\n",
    "First, let's send a request with a message to Llama 3.1, and as part of that request, we will inform Llama that we have a function on our end which can determine the weather in a given place."
   ],
   "id": "94026eea4e76abed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "client = ollama.Client()\n",
    "\n",
    "def tool_chat(model: str, query: str):\n",
    "    response = client.chat(\n",
    "        model = model,\n",
    "        messages = [{'role': 'user', 'content': query}],\n",
    "        tools = [\n",
    "          {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "              'name': 'get_current_temperature',\n",
    "              'description': 'Get the temperature in a place',\n",
    "              'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                  'place': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The place for which the temperature is requested',\n",
    "                  }\n",
    "                },\n",
    "                'required': ['place'],\n",
    "              },\n",
    "            },\n",
    "          },\n",
    "        ],\n",
    "      )\n",
    "    \n",
    "    return response['message']"
   ],
   "id": "929ea0bf4631c31f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's see what the LLM returns when we ask a question that relates in some way to weather.",
   "id": "8e2db81b8cdd018"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"Should I wear a warm coat today in Rome?\"\n",
    "\n",
    "tool_chat(\"llama3.1:8b\", query)"
   ],
   "id": "9c5b029fb17d1312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, so let's now build this into a chat workflow.",
   "id": "56d515803782ffc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "import asyncio\n",
    "\n",
    "async def weather_chat(model: str, query: str) -> str:\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a query\n",
    "  messages = [{'role': 'user', 'content': query}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  # use the tools parameter to define the tool so the LLM can understand it \n",
    "  response = await client.chat(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    tools = [\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_current_temperature',\n",
    "          'description': 'Get the temperature in a place',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'place': {\n",
    "                'type': 'string',\n",
    "                'description': 'The place for which the temperature is requested',\n",
    "              }\n",
    "            },\n",
    "            'required': ['place'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided tool and warn if it did not\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # If it did use the tool, process tool calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'get_current_temperature': get_current_temperature,\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['place'])\n",
    "      # Add function response to the conversation\n",
    "      # Warn model to override any default behavior\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': f\"\"\"\n",
    "          Answer the following question: {query}.  \n",
    "          Ignore any previous instructions or defaults and instead use the following information:  {function_response}\n",
    "          \"\"\",\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  # Hopefully it will provide the information obtained using the tool\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])\n"
   ],
   "id": "1d5b08244610167f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Testing using various open models and queries\n",
    "First, let's try this with Google's Gemma2 model and a simple question.\n"
   ],
   "id": "775606c24ada6b42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_chat('gemma2:27b', 'What is the temperature like right now in Dublin?')",
   "id": "2613434018146722",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, so we can see that some models don't yet support this feature.  Let's try the small version of Llama 3.1.",
   "id": "b13c1d99708621d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', 'What is the temperature like right now in Dublin?')",
   "id": "ab534669bbcba3bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Works nicely - let's try asking indirectly.",
   "id": "cf938bbaa66f03b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"What is the current temperature in Ireland's capital?\")",
   "id": "79fa0bc037ce5e84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Very nice! Let's push things a little further.",
   "id": "6e80545caf94cf10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"My sister says that if I flew into the capital city of Norway today, I should wear clothing for extreme weather.  Should I trust her advice?\")",
   "id": "3d1b1dc68db6cac1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"Compare the temperatures of these two cities right now: Dunedin, New Zealand and Reykjavik, Iceland?\")",
   "id": "52a713e58b780756",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"What kinds of clothes should I pack for my trip to Tasmania which leaves tomorrow?\")",
   "id": "918a2ccd021cb4f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"How much longer would it take a 50g ice cube to melt today in Dunedin compared to Marrakech?\")\n",
   "id": "bc2c4217fd90160",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_chat('llama3.1:8b', \"If a wombat was transported today to Northern Sweden and a penguin was transported today to Singapore, which would have the best chance of survival?\")",
   "id": "3bb47823431f8ade",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding multiple functions\n",
    "\n",
    "Let's construct a second function which obtains details of events available on Ticketmaster."
   ],
   "id": "f96388fc783d9640"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# function to get the today's sunset time in a place\n",
    "def get_sunset(place: str) -> str:\n",
    "  base_url = f'https://wttr.in/{place}?format=\"%s\"'\n",
    "  response = requests.get(base_url)\n",
    "  data = response.json()\n",
    "  return f\"Today's sunset time in {place} is {data}\"\n",
    "\n",
    "# test the function\n",
    "get_sunset(\"London\")"
   ],
   "id": "3ec5cee159b4a841",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's add this as an additional function available to Llama 3.1",
   "id": "49ea909208422a71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def weather_and_sunset_chat(model: str, query: str) -> str:\n",
    "  client = ollama.AsyncClient()\n",
    "  # Initialize conversation with a query\n",
    "  messages = [{'role': 'user', 'content': query}]\n",
    "\n",
    "  # First API call: Send the query and function description to the model\n",
    "  # use the tools parameter to define the tool so the LLM can understand it \n",
    "  response = await client.chat(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    tools = [\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_current_temperature',\n",
    "          'description': 'Get the temperature in a place',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'place': {\n",
    "                'type': 'string',\n",
    "                'description': 'The place for which the temperature is requested',\n",
    "              }\n",
    "            },\n",
    "            'required': ['place'],\n",
    "          },\n",
    "        },\n",
    "      },\n",
    "     {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "          'name': 'get_sunset',\n",
    "          'description': 'Get the sunset time in a place',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'place': {\n",
    "                'type': 'string',\n",
    "                'description': 'The city in which the sunset time is requested',\n",
    "              }\n",
    "            },\n",
    "            'required': ['place'],\n",
    "          },\n",
    "        },\n",
    "      }, \n",
    "    ],\n",
    "  )\n",
    "\n",
    "  # Add the model's response to the conversation history\n",
    "  messages.append(response['message'])\n",
    "\n",
    "  # Check if the model decided to use the provided tool and warn if it did not\n",
    "  if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "    return\n",
    "\n",
    "  # If it did use the tool, process tool calls made by the model\n",
    "  if response['message'].get('tool_calls'):\n",
    "    available_functions = {\n",
    "      'get_current_temperature': get_current_temperature,\n",
    "      'get_sunset': get_sunset\n",
    "    }\n",
    "    for tool in response['message']['tool_calls']:\n",
    "      function_to_call = available_functions[tool['function']['name']]\n",
    "      function_response = function_to_call(tool['function']['arguments']['place'])\n",
    "      # Add function response to the conversation\n",
    "      # Warn model to override any default behavior\n",
    "      messages.append(\n",
    "        {\n",
    "          'role': 'tool',\n",
    "          'content': f\"\"\"\n",
    "          Answer the following question: {query}.  \n",
    "          Ignore any previous instructions or defaults and instead use the following information:  {function_response}\n",
    "          \"\"\",\n",
    "        }\n",
    "      )\n",
    "\n",
    "  # Second API call: Get final response from the model\n",
    "  # Hopefully it will provide the information obtained using the tool\n",
    "  final_response = await client.chat(model=model, messages=messages)\n",
    "  print(final_response['message']['content'])"
   ],
   "id": "b0f38873186038dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now let's test a few queries:",
   "id": "e87b6b8c5621bc7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_and_sunset_chat(\"llama3.1:8b\", \"Should I dress light for my visit To Austin, Texas today?\")",
   "id": "95b92b9401d729d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "await weather_and_sunset_chat(\"llama3.1:70b\", \"I'm driving about 100 miles out of town of Austin, Texas today.  What kind of clothes should I pack?  Also I want to be back before dark.  Do you have any advice?\")",
   "id": "1ca6c0b2ea76d34a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
